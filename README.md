# ðŸ§  Running LLMs Locally with LangChain + Hugging Face

This project demonstrates how to run **Large Language Models (LLMs)** locally using **LangChain** in combination with the **Hugging Face Hub**, directly from environments like **Google Colab**. The aim is to showcase how open-weight models like Mistral, LLaMA, and Falcon can be invoked and tested for prompt-based generation using LangChainâ€™s integration capabilities.

---

## ðŸ“Œ Project Objective

To explore and test how open-source LLMs can be run **locally** or via **Hugging Face Inference Endpoints** using the `langchain-huggingface` wrapper. The project also demonstrates best practices in authentication, prompt structuring, and runtime behavior of popular instruction-tuned models.

---

## ðŸš€ What I Did

### ðŸ” Token Authentication
- Stored Hugging Face access token securely using `google.colab.userdata` in Colab.
- Passed the token to LangChain and Hugging Face APIs using Pythonâ€™s `os.environ`.

### ðŸ”— Model Integration
- Used the `HuggingFaceEndpoint` wrapper from `langchain_huggingface` to connect to models hosted on the Hugging Face Hub.
- Integrated models like:
  - `mistralai/Mistral-7B-Instruct-v0.3`
  - `meta-llama/Llama-2-7b-chat-hf`
  - `tiiuae/falcon-7b-instruct`

### âš™ï¸ LangChain Usage
- Configured the LLMs with key parameters like `max_length` and `temperature`.
- Called `.invoke()` to send prompts and receive model completions.

### ðŸ§ª Prompt Testing
- Sent multiple prompts to evaluate model responses.
- Adjusted parameters to control verbosity and creativity of the output.

### ðŸ’» Google Colab Setup
- Installed required libraries: `langchain-huggingface`, `huggingface_hub`, `transformers`, `accelerate`, `bitsandbytes`.
- Verified GPU compatibility to support faster execution of model queries.

### ðŸ§µ Toolchain Integration
- Used `transformers` for backend operations and model control.
- Integrated `bitsandbytes` and `accelerate` to enable efficient memory usage for larger models.
- Cached models locally inside Colab environment to improve speed.

---

## ðŸ”§ Technologies Used

- **LangChain**
- **Hugging Face Hub**
- **Transformers**
- **Accelerate**
- **Bitsandbytes**
- **Google Colab**
- **Python**

---

## ðŸ§ª Models Explored

- `mistralai/Mistral-7B-Instruct-v0.3`
- `meta-llama/Llama-2-7b-chat-hf`
- `tiiuae/falcon-7b-instruct`

> You can replace these with any Hugging Face model that supports `text-generation` and open weights.

---

## ðŸ“¦ Installation (in Google Colab)

```bash
!pip install langchain-huggingface
!pip install huggingface_hub
!pip install transformers
!pip install accelerate
!pip install bitsandbytes

## âœ… Features

- âœ… End-to-end LLM setup using Hugging Face + LangChain  
- ðŸ” Secure token handling via Colab `userdata`  
- âœï¸ Prompt-based invocation and output formatting  
- âš¡ Real-time interaction with instruction-tuned open-source models  
- â˜ï¸ Run-time experimentation in cloud environment (Google Colab)  

---

## ðŸ¤– Future Improvements

- ðŸ§© Extend with LangChain tools like chains and agents  
- ðŸŒ Add streaming output via WebSocket or Gradio interface  
- ðŸ–¥ï¸ Support for local inference using `transformers.pipeline()` and CPU/GPU fallback  
- ðŸ§ª UI to test prompts dynamically  

---

## ðŸ“« Contact

**Shiva Charan Pailla**  
ðŸ“§ shivacharanpailla@gmail.com  
 

---

## ðŸ” Token Safety Note

> Please note: I have not included screenshots or runtime proofs in this public repository to avoid exposing sensitive Hugging Face tokens, which could be misused by others.  
> For any verification, Iâ€™m happy to demo the functionality securely upon request.

---

## ðŸ™Œ Acknowledgements

Huge thanks to the amazing communities behind:

- ðŸ¤— **Hugging Face** â€“ for hosting accessible open-weight models  
- ðŸ¦œ **LangChain** â€“ for simplifying prompt interaction and LLM orchestration  
- ðŸ§ª **Google Colab** â€“ for making quick prototyping with GPUs easy and accessible  

